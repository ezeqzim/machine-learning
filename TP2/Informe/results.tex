\section{Resultados}

Antes de comenzar, realizamos pruebas sobre el valor inicial de una nueva entrada en el diccionario $\{estado, accion\} \rightarrow Q valor$.

Arrancamos con un valor de 1.0, se ve un aprendizaje consistente.

Colocando un valor random entre 0 y 1, el entrenamiento es err\'atico.

Usando 0.0, funciona igual que con 1.0, pero con un win\_percentage considerablemente mayor.

De esta manera, en todos los experimentos utilizamos 0.0 como valor inicial de Q para cada estado y acci\'on correspondiente.

Luego realizamos un gridsearch sobre los tres hiperpar\'ametros de la t\'ecnica de Q Leaning. Los resultados fueron los siguientes:

\textcolor{red}{FRAN}

Basamos luego la experimentaci\'on en analizar emp\'iricamente las siguientes preguntas:

\begin{itemize}
  \item Qu\'e tan r\'apido aprende un Q-Learning frente a un Random?
  \item Si un Q-Learning entrena a la par de otro, le ser\'a de ayuda al enfrentar a un Random?
  \item Ser\'a bueno empezar a entrenar con alguien que ya tiene experiencia para ser a\'un mejor?
\end{itemize}

\input{experiment1}
\newpage
\input{experiment2}
\newpage
\input{experiment3}
