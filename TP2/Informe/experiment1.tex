\subsection{¿Aprende el Q-Learner contra un Random?}

La intención de este experimento fue observar si el Q-Learner aprende, es decir, si tras un número considerable de partidas jugadas contra un Random, consigue aumentar su porcentaje de partidas ganadas.

Para probar esto, se entrenó a un Q-Learner contra un Random durante un millón de iteraciones, graficando los resultados.

Se conjeturó que el Q-Learner aprenderá, a medida vayan pasando las iteraciones, y por lo tanto el porcentaje de partidas ganadas aumentará. Se estimó que esta mejoría se hará visible tras aproximadamente 200000 iteraciones.

Se presentan a continuación los resultados del experimento.

\ig{Imagenes/Results/exp1/zero_any_move/zero_any_move.png}{Porcentaje de victorias de Q-Learner en ventanas de 500 partidas}

\ig{Imagenes/Results/exp1/zero_any_move/zero_any_move_acum.png}{Porcentaje de victorias totales de Q-Learner a lo largo de las iteraciones}

Se puede observar que conforme avanzan las iteraciones, el Q-Learner gana cada vez m\'as partidas (en los intervalos de 500 partidas), con lo cual el porcentaje acumulado aumenta consistentemente.

La curva de aprendizaje es creciente, pero presenta una un comportamiento asintótico.

Se observar que tras 200000 iteraciones, el Q-Learner gana aproximadamente el 65\% de las partidas, y tras las otras 800000 iteraciones, el porcentaje está alrededor del 75\%.

Para entender el comportamiento del Q-Learner, se jugaron varias partidas contra él para ver qué estrategia utiliza.

Su estrategia consistió en apilar fichas en la columna de más a la derecha. Si uno intenta bloquearlo, sigue poniendo fichas en esa columna hasta que se llene. Tras llenar esa columna, ya no es posible determinar con seguridad su comportamiento.
Debido a que está jugando contra un Random, se estima que esta estrategia le otorga la victoria un número interesante de veces.

Se conjeturó que la gran mayoría de las veces que gana, gana utilizando dicha estrategia, por lo que otorga un gran valor de $Q$ a esos estados y transiciones. Además, el Random no es un oponente interesante y suponemos que no le da la oportunidad de probar nuevos estados.

Realizando un análisis de por qué su aprendizaje comienza a decrecer, se concluyó a que se debe, en parte, al coeficiente de exploración, es decir, si explora corre el riesgo de  perder.

Para validar dicha hipótesis, se utilizó al Q-Learner ya entrenado y se le prohibió explorar, para luego enfrentarlo al Random.

Los resultados fueron los siguientes.

\ig{Imagenes/Results/exp1/normal_validation/normal_validation_validation.png}{Porcentaje de victorias de Q-Learner en ventanas de 500 partidas con $\varepsilon = 0$}

\ig{Imagenes/Results/exp1/normal_validation/normal_validation_validation_acum.png}{Porcentaje total de victorias de Q-Learner a lo largo de las iteraciones con $\varepsilon = 0$}

Como se supuso, una vez desactivada la exploración, su desmepeño aumentó considerablemente, ganando por encima del 90\% de las veces.

Cuando aún tenía el $\varepsilon$ mayor a cero, la exploración hacía que se desvíe de su estrategia. Una vez se desvía de su estrategia ganadora, se puede observar que su desempeño empeora.

La hipótesis que se plantea entonces es que probablemente obtenga la mayor cantidad de victorias siguiendo con la estrategia de ubicar fichas en la columna de la derecha.

Por lo tanto, no está aprendiendo demasiado, dado que simplemente obtiene la victoria en muchas partidas utilizando una estrategia poco inteligente. Esto supone que el aprendizaje se realiza lentamente, ya que se está enfrentando contra un jugador muy poco inteligente.

En vista de estos resultados, se decidió ver qué ocurre si se lo entrena contra un oponente más inteligente.
