\section{Conclusiones}
En primer lugar, se concluyó que las mediciones se deben realizar con cautela, cuando se mide el aprendizaje contra jugadores Random, ya que pueden no estar correlacionados el aprendizaje con las victorias contra dichos jugadores, como se observó con el BoboPlayer. 

Lo más apropiado es enfrentarlo contra otros Q-Learners, o enfrentarlo manualmente, y medir el porcentaje de victorias.\\

Luego, se notó que los resultados obtenidos contra jugadores humanos no fueron buenos. El Q-Learner nunca supuso un desafío ni mostró señales de aprendizaje para bloquear y evitar victorias del oponente. Se concluyó que si se entrenasen dos Q-Learners durante un número suficiente de iteraciones (es decir, con cantidad de iteraciones tendiendo al infinito), éstos recorrerían todos los estados posibles, lo que los convertiría en un oponente interesante. 

En contraposición a lo mencionado, se supuso que si se dejase un Q-Learner entrenando contra un Random un número suficiente de iteraciones, el Q-Learner no se convertiría en un oponente desafiante para un jugador humano, dado que el Random no le proporciona la oportunidad de visitar todos los estados y en la gran mayoría de esas iteraciones, ganaría simplemente utilizando una mala estrategia como la del BoboPlayer.\\

Es interesante pensar qué sucedería si se pudiese entrenar al Q-Learner contra jugadores humanos, es decir, hacerlo jugar contra distintos jugadores humanos por millones de iteraciones. Se conjeturó que ese entrenamiento sería mucho más eficiente y finalmente aprendería a jugar de forma tal que resulte desafiante contra un jugador humano.\\

Es necesario mencionar, que se supuso probable que agregando más lógica al algoritmo de Q-Learning, el Q-Learner obtenga resultados notablemente mejores. Es posible que si se le diése una recompensa a los estados donde acaba de bloquear, se obtenan mejores resultados. 

Sin embargo, el enfoque de esta investigación fue intentar que el Q-Learner aprenda con la menor lógica posible, para poder observar qué resultados se obtenían partiendo de un algoritmo de Q-Learning simple.
